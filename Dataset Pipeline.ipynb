{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import trackml\n",
    "from trackml.dataset import load_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \"\"\"\n",
    "    Preprocessor class will parse dataset under required specifications.\n",
    "    \"\"\"\n",
    "    def __init__(self, cells_data, hits_data, particles_data, truth_data):\n",
    "        self.cells = pd.read_csv(cells_data)\n",
    "        self.hits  = pd.read_csv(hits_data)\n",
    "        self.particles = pd.read_csv(particles_data)\n",
    "        self.truth = pd.read_csv(truth_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits, cells, particles, truth = load_event('/Users/Eden/Desktop/Datasets/train_100_events/event000001000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hit_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>volume_id</th>\n",
       "      <th>layer_id</th>\n",
       "      <th>module_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-64.409897</td>\n",
       "      <td>-7.163700</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-55.336102</td>\n",
       "      <td>0.635342</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-83.830498</td>\n",
       "      <td>-1.143010</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-96.109100</td>\n",
       "      <td>-8.241030</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-62.673599</td>\n",
       "      <td>-9.371200</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120934</th>\n",
       "      <td>120935</td>\n",
       "      <td>-763.862976</td>\n",
       "      <td>51.569401</td>\n",
       "      <td>2944.5</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120935</th>\n",
       "      <td>120936</td>\n",
       "      <td>-808.705017</td>\n",
       "      <td>3.459260</td>\n",
       "      <td>2944.5</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120936</th>\n",
       "      <td>120937</td>\n",
       "      <td>-982.935974</td>\n",
       "      <td>41.460899</td>\n",
       "      <td>2952.5</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120937</th>\n",
       "      <td>120938</td>\n",
       "      <td>-942.698975</td>\n",
       "      <td>18.489100</td>\n",
       "      <td>2952.5</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120938</th>\n",
       "      <td>120939</td>\n",
       "      <td>-922.890015</td>\n",
       "      <td>2.092850</td>\n",
       "      <td>2952.5</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120939 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        hit_id           x          y       z  volume_id  layer_id  module_id\n",
       "0            1  -64.409897  -7.163700 -1502.5          7         2          1\n",
       "1            2  -55.336102   0.635342 -1502.5          7         2          1\n",
       "2            3  -83.830498  -1.143010 -1502.5          7         2          1\n",
       "3            4  -96.109100  -8.241030 -1502.5          7         2          1\n",
       "4            5  -62.673599  -9.371200 -1502.5          7         2          1\n",
       "...        ...         ...        ...     ...        ...       ...        ...\n",
       "120934  120935 -763.862976  51.569401  2944.5         18        12         97\n",
       "120935  120936 -808.705017   3.459260  2944.5         18        12         97\n",
       "120936  120937 -982.935974  41.460899  2952.5         18        12         98\n",
       "120937  120938 -942.698975  18.489100  2952.5         18        12         98\n",
       "120938  120939 -922.890015   2.092850  2952.5         18        12         98\n",
       "\n",
       "[120939 rows x 7 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "6437\n",
      "[[0.03144621 0.8504313  0.0595952 ]\n",
      " [0.07216    0.84666234 0.136875  ]\n",
      " [0.11557334 0.8430469  0.21927   ]\n",
      " ...\n",
      " [0.65818244 0.7734815  0.0616    ]\n",
      " [0.81695163 0.787495   0.076     ]\n",
      " [1.0168136  0.80558014 0.0976    ]]\n",
      "y\n",
      "10670\n",
      "[1. 0. 0. ... 1. 1. 1.]\n",
      "Ri_rows\n",
      "10670\n",
      "[   1    1    1 ... 6434 6435 6436]\n",
      "Ri_cols\n",
      "10670\n",
      "[    0   789  1535 ...  9747 10252 10669]\n",
      "Ro_rows\n",
      "10670\n",
      "[   0    0    0 ... 6433 6434 6435]\n",
      "Ro_cols\n",
      "10670\n",
      "[    0     1     2 ...  9747 10252 10669]\n"
     ]
    }
   ],
   "source": [
    "from numpy import load\n",
    "\n",
    "data = load('/Users/Eden/Desktop/event000001000_g000.npz')\n",
    "lst = data.files\n",
    "for item in lst:\n",
    "    print(item)\n",
    "    print(len(data[item]))\n",
    "    print(data[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Eden/opt/anaconda3/envs/quantum-computing/lib/python3.7/site-packages/pandas/core/internals/construction.py:540: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "npz = np.load('/Users/Eden/Desktop/event000001000_g000.npz')\n",
    "df= pd.DataFrame.from_dict({item: npz[item] for item in npz.files}, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Pipeline.ipynb             QEN Circuit and QNoN Circuit.ipynb\r\n",
      "Open_Hackathon.md                  \u001b[34mpreprocessing\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'preprocessing.datasets.graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-ef58d436b645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'preprocessing.datasets.graph'"
     ]
    }
   ],
   "source": [
    "# Inherited from HepTrkX: https://github.com/HEPTrkX/heptrkx-gnn-tracking\n",
    "\n",
    "\"\"\"\n",
    "Data preparation script for GNN tracking.\n",
    "\n",
    "This script processes the TrackML dataset and produces graph data on disk.\n",
    "\"\"\"\n",
    "\n",
    "# System\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "# Externals\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import trackml.dataset\n",
    "\n",
    "# Locals\n",
    "from preprocessing.datasets.graph import Graph, save_graphs\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse command line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser('prepare.py')\n",
    "    add_arg = parser.add_argument\n",
    "    add_arg('config', nargs='?', default='configs/prepare_trackml.yaml')\n",
    "    add_arg('--n-workers', type=int, default=1)\n",
    "    add_arg('--task', type=int, default=0)\n",
    "    add_arg('--n-tasks', type=int, default=1)\n",
    "    add_arg('-v', '--verbose', action='store_true')\n",
    "    add_arg('--show-config', action='store_true')\n",
    "    add_arg('--interactive', action='store_true')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def calc_dphi(phi1, phi2):\n",
    "    \"\"\"Computes phi2-phi1 given in range [-pi,pi]\"\"\"\n",
    "    dphi = phi2 - phi1\n",
    "    dphi[dphi > np.pi] -= 2*np.pi\n",
    "    dphi[dphi < -np.pi] += 2*np.pi\n",
    "    return dphi\n",
    "\n",
    "def calc_eta(r, z):\n",
    "    theta = np.arctan2(r, z)\n",
    "    return -1. * np.log(np.tan(theta / 2.))\n",
    "\n",
    "def select_segments(hits1, hits2, phi_slope_max, z0_max):\n",
    "    \"\"\"\n",
    "    Construct a list of selected segments from the pairings\n",
    "    between hits1 and hits2, filtered with the specified\n",
    "    phi slope and z0 criteria.\n",
    "\n",
    "    Returns: pd DataFrame of (index_1, index_2), corresponding to the\n",
    "    DataFrame hit label-indices in hits1 and hits2, respectively.\n",
    "    \"\"\"\n",
    "    # Start with all possible pairs of hits\n",
    "    keys = ['evtid', 'r', 'phi', 'z']\n",
    "    hit_pairs = hits1[keys].reset_index().merge(\n",
    "        hits2[keys].reset_index(), on='evtid', suffixes=('_1', '_2'))\n",
    "    # Compute line through the points\n",
    "    dphi = calc_dphi(hit_pairs.phi_1, hit_pairs.phi_2)\n",
    "    dz = hit_pairs.z_2 - hit_pairs.z_1\n",
    "    dr = hit_pairs.r_2 - hit_pairs.r_1\n",
    "    phi_slope = dphi / dr\n",
    "    z0 = hit_pairs.z_1 - hit_pairs.r_1 * dz / dr\n",
    "    # Filter segments according to criteria\n",
    "    good_seg_mask = (phi_slope.abs() < phi_slope_max) & (z0.abs() < z0_max)\n",
    "    return hit_pairs[['index_1', 'index_2']][good_seg_mask]\n",
    "\n",
    "def construct_graph(hits, layer_pairs,\n",
    "                    phi_slope_max, z0_max,\n",
    "                    feature_names, feature_scale):\n",
    "    \"\"\"Construct one graph (e.g. from one event)\"\"\"\n",
    "\n",
    "    # Loop over layer pairs and construct segments\n",
    "    layer_groups = hits.groupby('layer')\n",
    "    segments = []\n",
    "    for (layer1, layer2) in layer_pairs:\n",
    "        # Find and join all hit pairs\n",
    "        try:\n",
    "            hits1 = layer_groups.get_group(layer1)\n",
    "            hits2 = layer_groups.get_group(layer2)\n",
    "        # If an event has no hits on a layer, we get a KeyError.\n",
    "        # In that case we just skip to the next layer pair\n",
    "        except KeyError as e:\n",
    "            logging.info('skipping empty layer: %s' % e)\n",
    "            continue\n",
    "        # Construct the segments\n",
    "        segments.append(select_segments(hits1, hits2, phi_slope_max, z0_max))\n",
    "    # Combine segments from all layer pairs\n",
    "    segments = pd.concat(segments)\n",
    "\n",
    "    # Prepare the graph matrices\n",
    "    n_hits = hits.shape[0]\n",
    "    n_edges = segments.shape[0]\n",
    "    X = (hits[feature_names].values / feature_scale).astype(np.float32)\n",
    "    Ri = np.zeros((n_hits, n_edges), dtype=np.uint8)\n",
    "    Ro = np.zeros((n_hits, n_edges), dtype=np.uint8)\n",
    "    y = np.zeros(n_edges, dtype=np.float32)\n",
    "\n",
    "    # We have the segments' hits given by dataframe label,\n",
    "    # so we need to translate into positional indices.\n",
    "    # Use a series to map hit label-index onto positional-index.\n",
    "    hit_idx = pd.Series(np.arange(n_hits), index=hits.index)\n",
    "    seg_start = hit_idx.loc[segments.index_1].values\n",
    "    seg_end = hit_idx.loc[segments.index_2].values\n",
    "\n",
    "    # Now we can fill the association matrices.\n",
    "    # Note that Ri maps hits onto their incoming edges,\n",
    "    # which are actually segment endings.\n",
    "    Ri[seg_end, np.arange(n_edges)] = 1\n",
    "    Ro[seg_start, np.arange(n_edges)] = 1\n",
    "    # Fill the segment labels\n",
    "    pid1 = hits.particle_id.loc[segments.index_1].values\n",
    "    pid2 = hits.particle_id.loc[segments.index_2].values\n",
    "    y[:] = (pid1 == pid2)\n",
    "    # Return a tuple of the results\n",
    "    return Graph(X, Ri, Ro, y)\n",
    "\n",
    "def select_hits(hits, truth, particles, pt_min=0):\n",
    "    # Barrel volume and layer ids\n",
    "    vlids = [(8,2), (8,4), (8,6), (8,8),\n",
    "             (13,2), (13,4), (13,6), (13,8),\n",
    "             (17,2), (17,4)]\n",
    "    n_det_layers = len(vlids)\n",
    "    # Select barrel layers and assign convenient layer number [0-9]\n",
    "    vlid_groups = hits.groupby(['volume_id', 'layer_id'])\n",
    "    hits = pd.concat([vlid_groups.get_group(vlids[i]).assign(layer=i)\n",
    "                      for i in range(n_det_layers)])\n",
    "    # Calculate particle transverse momentum\n",
    "    pt = np.sqrt(particles.px**2 + particles.py**2)\n",
    "    # True particle selection.\n",
    "    # Applies pt cut, removes all noise hits.\n",
    "    particles = particles[pt > pt_min]\n",
    "    truth = (truth[['hit_id', 'particle_id']]\n",
    "             .merge(particles[['particle_id']], on='particle_id'))\n",
    "    # Calculate derived hits variables\n",
    "    r = np.sqrt(hits.x**2 + hits.y**2)\n",
    "    phi = np.arctan2(hits.y, hits.x)\n",
    "    # Select the data columns we need\n",
    "    hits = (hits[['hit_id', 'z', 'layer']]\n",
    "            .assign(r=r, phi=phi)\n",
    "            .merge(truth[['hit_id', 'particle_id']], on='hit_id'))\n",
    "    # Remove duplicate hits\n",
    "    hits = hits.loc[\n",
    "        hits.groupby(['particle_id', 'layer'], as_index=False).r.idxmin()\n",
    "    ]\n",
    "    return hits\n",
    "\n",
    "def split_detector_sections(hits, phi_edges, eta_edges):\n",
    "    \"\"\"Split hits according to provided phi and eta boundaries.\"\"\"\n",
    "    hits_sections = []\n",
    "    # Loop over sections\n",
    "    for i in range(len(phi_edges) - 1):\n",
    "        phi_min, phi_max = phi_edges[i], phi_edges[i+1]\n",
    "        # Select hits in this phi section\n",
    "        phi_hits = hits[(hits.phi > phi_min) & (hits.phi < phi_max)]\n",
    "        # Center these hits on phi=0\n",
    "        centered_phi = phi_hits.phi - (phi_min + phi_max) / 2\n",
    "        phi_hits = phi_hits.assign(phi=centered_phi, phi_section=i)\n",
    "        for j in range(len(eta_edges) - 1):\n",
    "            eta_min, eta_max = eta_edges[j], eta_edges[j+1]\n",
    "            # Select hits in this eta section\n",
    "            eta = calc_eta(phi_hits.r, phi_hits.z)\n",
    "            sec_hits = phi_hits[(eta > eta_min) & (eta < eta_max)]\n",
    "            hits_sections.append(sec_hits.assign(eta_section=j))\n",
    "    return hits_sections\n",
    "\n",
    "def process_event(prefix, output_dir, pt_min, n_eta_sections, n_phi_sections,\n",
    "                  eta_range, phi_range, phi_slope_max, z0_max):\n",
    "    # Load the data\n",
    "    evtid = int(prefix[-9:])\n",
    "    logging.info('Event %i, loading data' % evtid)\n",
    "    hits, particles, truth = trackml.dataset.load_event(\n",
    "        prefix, parts=['hits', 'particles', 'truth'])\n",
    "\n",
    "    # Apply hit selection\n",
    "    logging.info('Event %i, selecting hits' % evtid)\n",
    "    hits = select_hits(hits, truth, particles, pt_min=pt_min).assign(evtid=evtid)\n",
    "\n",
    "    # Divide detector into sections\n",
    "    #phi_range = (-np.pi, np.pi)\n",
    "    phi_edges = np.linspace(*phi_range, num=n_phi_sections+1)\n",
    "    eta_edges = np.linspace(*eta_range, num=n_eta_sections+1)\n",
    "    hits_sections = split_detector_sections(hits, phi_edges, eta_edges)\n",
    "\n",
    "    # Graph features and scale\n",
    "    feature_names = ['r', 'phi', 'z']\n",
    "    feature_scale = np.array([1000., np.pi / n_phi_sections, 1000.])\n",
    "\n",
    "    # Define adjacent layers\n",
    "    n_det_layers = 10\n",
    "    l = np.arange(n_det_layers)\n",
    "    layer_pairs = np.stack([l[:-1], l[1:]], axis=1)\n",
    "\n",
    "    # Construct the graph\n",
    "    logging.info('Event %i, constructing graphs' % evtid)\n",
    "    graphs = [construct_graph(section_hits, layer_pairs=layer_pairs,\n",
    "                              phi_slope_max=phi_slope_max, z0_max=z0_max,\n",
    "                              feature_names=feature_names,\n",
    "                              feature_scale=feature_scale)\n",
    "              for section_hits in hits_sections]\n",
    "\n",
    "    # Write these graphs to the output directory\n",
    "    try:\n",
    "        base_prefix = os.path.basename(prefix)\n",
    "        filenames = [os.path.join(output_dir, '%s_g%03i' % (base_prefix, i))\n",
    "                     for i in range(len(graphs))]\n",
    "    except Exception as e:\n",
    "        logging.info(e)\n",
    "    logging.info('Event %i, writing graphs', evtid)\n",
    "    save_graphs(graphs, filenames)\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"Main function\"\"\"\n",
    "\n",
    "#     # Parse the command line\n",
    "#     args = parse_args()\n",
    "\n",
    "#     # Setup logging\n",
    "#     log_format = '%(asctime)s %(levelname)s %(message)s'\n",
    "#     log_level = logging.DEBUG if args.verbose else logging.INFO\n",
    "#     logging.basicConfig(level=log_level, format=log_format)\n",
    "#     logging.info('Initializing')\n",
    "#     if args.show_config:\n",
    "#         logging.info('Command line config: %s' % args)\n",
    "\n",
    "#     # Load configuration\n",
    "#     with open(args.config) as f:\n",
    "#         config = yaml.load(f)\n",
    "#     if args.task == 0:\n",
    "#         logging.info('Configuration: %s' % config)\n",
    "\n",
    "#     # Construct layer pairs from adjacent layer numbers\n",
    "#     layers = np.arange(10)\n",
    "#     layer_pairs = np.stack([layers[:-1], layers[1:]], axis=1)\n",
    "\n",
    "#     # Find the input files\n",
    "#     input_dir = config['input_dir']\n",
    "#     all_files = os.listdir(input_dir)\n",
    "#     suffix = '-hits.csv'\n",
    "#     file_prefixes = sorted(os.path.join(input_dir, f.replace(suffix, ''))\n",
    "#                            for f in all_files if f.endswith(suffix))\n",
    "#     file_prefixes = file_prefixes[:config['n_files']]\n",
    "\n",
    "#     # Split the input files by number of tasks and select my chunk only\n",
    "#     file_prefixes = np.array_split(file_prefixes, args.n_tasks)[args.task]\n",
    "\n",
    "#     # Prepare output\n",
    "#     output_dir = os.path.expandvars(config['output_dir'])\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     logging.info('Writing outputs to ' + output_dir)\n",
    "\n",
    "#     # Process input files with a worker pool\n",
    "#     with mp.Pool(processes=args.n_workers) as pool:\n",
    "#         process_func = partial(process_event, output_dir=output_dir,\n",
    "#                                phi_range=(-np.pi, np.pi), **config['selection'])\n",
    "#         pool.map(process_func, file_prefixes)\n",
    "\n",
    "#     # Drop to IPython interactive shell\n",
    "#     if args.interactive:\n",
    "#         logging.info('Starting IPython interactive session')\n",
    "#         import IPython\n",
    "#         IPython.embed()\n",
    "\n",
    "#     logging.info('All done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
